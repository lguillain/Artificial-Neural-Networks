{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 2: Melody generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "To develop a model employing ANN on real-world data requires going through several major steps, each of which with important design choices that directly impact the end results. In this project, we guide you through these choices starting from a large database of [Irish folk melodies](https://github.com/IraKorshunova/folk-rnn/tree/master/data) to your own model of symbolic music composition. \n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"recurrent neural networks\", \"LSTM\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will be guided through a data processing procedure and understand the importance of design choices in ANN modeling\n",
    "- You will learn how to define recurrent neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"overfitting\", \"LSTM network\", and \"Generative model\".\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook. Please ensure that your notebook is fully executed before handing it in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and imports\n",
    "\n",
    "For your convenience we import some libraries and provide some functions below. Some libraries (`midi` and `music21==5.1.0`) should be added. If you work with the docker image, pull it `docker pull zifeo/artificial-neural-networks:cpu` before to get the latest version where these libraries are added and stop/relaunch the container (as stated in the instructions https://github.com/zifeo/artificial-neural-networks). If you work locally, be sure to import them. If you do not find the midi library for python3, install it with `python setup.py install` inside the `python3-midi` folder after cloning https://github.com/louisabraham/python3-midi.git.\n",
    "\n",
    "Finally, fill in your sciper number(s) for the random seed and simply run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "sciper = {'student_1': 0, \n",
    "          'student_2': 0}\n",
    "seed = sciper['student_1']+sciper['student_2']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "\n",
    "plt.rcParams['font.size'] = 28\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "c = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.rcParams['figure.figsize'] = 8, 4\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Masking, TimeDistributed, Dense, Concatenate, Dropout, LSTM, GRU, SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import midi\n",
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMelody(P, T, dictionaries, PrP=None, PrT=None, H=None):\n",
    "    \"\"\"\n",
    "    P: The pitch array (integer representation) or matrix (one-hot encoding)\n",
    "    T: The duration array (integer representation) or matrix (one-hot encoding)\n",
    "    dictionaries: The dictionaries relating integer to a pitch or duration\n",
    "    PrP: (Optional) If provided with PrT, plot the transition probabilities\n",
    "    PrT: (Optional) If provided with PrP, plot the transition probabilities\n",
    "    H: (Optional) Add a plot with the hidden state dynamic sorted by frequency\n",
    "    \"\"\"\n",
    "    P = np.asarray(P)\n",
    "    T = np.asarray(T)\n",
    "    if len(P.shape) == 1:#if array\n",
    "        P = np_utils.to_categorical(P, len(dictionaries['P']))\n",
    "    if len(T.shape) == 1:#if array\n",
    "        T = np_utils.to_categorical(T, len(dictionaries['T']))\n",
    "    \n",
    "    xlim = [-1,np.where(P==1)[0][-1]+1]\n",
    "    \n",
    "    activeidxes = np.where(np.sum(P, axis=0)>1.)[0]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    if PrP is not None:\n",
    "        targets = np.where(P[:,activeidxes]==1)\n",
    "        plt.imshow(PrP[:,activeidxes].T, origin='lower', aspect='auto', interpolation='nearest', cmap=plt.cm.gray_r, vmin=0, vmax=1)\n",
    "        plt.scatter(targets[0]-1,targets[1], color='red',s=10, marker='.') \n",
    "    else:\n",
    "        plt.imshow(P[:,activeidxes].T, origin='lower', aspect='auto', interpolation='nearest', cmap=plt.cm.gray_r, vmin=0, vmax=1)\n",
    "    plt.ylim([-0.5,len(activeidxes)-0.5])\n",
    "    plt.yticks(range(len(activeidxes)),[dictionaries[\"P\"][i] for i in activeidxes])\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"$n$\")\n",
    "    plt.ylabel(\"$P[n]$\")\n",
    "    plt.xlim(xlim)\n",
    "    plt.show()\n",
    "    \n",
    "    activeidxes = np.where(np.sum(T, axis=0)>1.)[0]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    if PrT is not None:\n",
    "        targets = np.where(T[:,activeidxes]==1)\n",
    "        plt.imshow(PrT[:,activeidxes].T, origin='lower', aspect='auto', interpolation='nearest', cmap=plt.cm.gray_r, vmin=0, vmax=1)\n",
    "        plt.scatter(targets[0]-1,targets[1], color='red',s=10, marker='.')\n",
    "    else:\n",
    "        plt.imshow(T[:,activeidxes].T, origin='lower', aspect='auto', interpolation='nearest', cmap=plt.cm.gray_r, vmin=0, vmax=1)\n",
    "    plt.ylim([-0.5,len(activeidxes)-0.5])\n",
    "    plt.yticks(range(len(activeidxes)),[dictionaries[\"T\"][i] for i in activeidxes])\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"$n$\")\n",
    "    plt.ylabel(\"$T[n]$\")\n",
    "    plt.xlim(xlim)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if H is not None:\n",
    "        diff = np.mean([[abs(j-i) for i,j in zip(H[:xlim[1]-1,k], H[1:xlim[1],k])] for k in range(H.shape[1])], axis=1)\n",
    "        sortidx = np.argsort(-diff)\n",
    "        H = H[:,sortidx]\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.imshow(H.T, origin='lower', aspect='auto', interpolation='nearest', cmap=plt.cm.seismic, vmin=-1, vmax=1)\n",
    "        plt.xlim([-1,np.where(T==1)[0][-1]+1])\n",
    "        plt.tight_layout()\n",
    "        plt.xlabel(\"$n$\")\n",
    "        plt.ylabel(\"$H[n]$\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plotLearningCurves(History):\n",
    "    \n",
    "    log = History.history\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(log['YP_loss'], '--', c=c[0], label='Pitch train loss')\n",
    "    plt.plot(log['val_YP_loss'], c=c[0], label='Pitch val loss')\n",
    "    \n",
    "    plt.plot(log['YT_loss'], '--', c=c[1], label='Dur train loss')\n",
    "    plt.plot(log['val_YT_loss'], c=c[1], label='Dur val loss')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(log['YP_acc'], '--', c=c[0], label='Pitch train acc')\n",
    "    plt.plot(log['val_YP_acc'], c=c[0], label='Pitch val acc')\n",
    "    \n",
    "    plt.plot(log['YT_acc'], '--', c=c[1], label='Dur train acc')\n",
    "    plt.plot(log['val_YT_acc'], c=c[1], label='Dur val acc')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "def sample(preds, temperature=1.):\n",
    "    \"\"\"Helper function to sample an index from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def writeMIDI(xP, xT, dictionaries, label):\n",
    "\n",
    "    # Instantiate a MIDI Pattern (contains a list of tracks)\n",
    "    pattern = midi.Pattern(format = 0, resolution=480)\n",
    "    # Instantiate a MIDI Track (contains a list of MIDI events)\n",
    "    track = midi.Track()\n",
    "    # Append the track to the pattern\n",
    "    pattern.append(track)\n",
    "    Events = []\n",
    "    \n",
    "    pseq = [dictionaries['P'][p] for p in xP]\n",
    "    tseq = [dictionaries['T'][t] for t in xT]\n",
    "    for t, p in zip(tseq, pseq):\n",
    "        e = midi.NoteOnEvent(tick=0, velocity=90, pitch=p)\n",
    "        track.append(e)\n",
    "        e = midi.NoteOffEvent(tick=t, velocity=0, pitch=p)\n",
    "        track.append(e)\n",
    "        \n",
    "    eot = midi.EndOfTrackEvent(tick=1)\n",
    "    track.append(eot)\n",
    "    # Save the pattern to disk\n",
    "    midi.write_midifile(label, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Data processing\n",
    "### Description\n",
    "\n",
    "For a real world application of ANNs, the original data should be translated into a representation understandable by an ANN. This step is often neglected but is of outmost importance. In order for you to better understand how ANN are applied to real world problems, the first step of this miniproject will guide you through a possible representation. \n",
    "\n",
    "Note that in general, your representation should be normalized in order to obtain homogeneous data from which a structure can be extracted by your ANN model. Importantly, all processing steps toward your representation should not (or minimally) distort your original data.\n",
    "\n",
    "To represent MIDI files, we provide you with a Python MIDI library. You should then\n",
    "\n",
    "0. Download the MIDI tar file from [this link](https://github.com/IraKorshunova/folk-rnn/tree/master/data) and put it in a folder named \"chorpus\" at the same level of this notebook.\n",
    "1. Parse the chorpus to extract the label of (a random subset of) the data (code is provided)\n",
    "2. Read each MIDI file in the subset with the midi [library](https://github.com/louisabraham/python3-midi)\n",
    "3. For each data sequence extract the sequence of MIDI notes defined by two dimensions (\"P\": the sequence of note pitches (integer between 0 and 127 representing a piano key), \"T\": the sequence of note durations.\n",
    "4. Select two melodies and display them both as sequences of MIDI events and sequence of notes in two dimensions. Control that your representation is correct.\n",
    "\n",
    "For step 3, you should learn how MIDI files are constructed. Below, we give a minimal explanation and some code for this particular project. For a more complete explanation, look for MIDI in your favorite search engine. Given this dataset, you can observe that it is monophonic (one note at a time) and there's a small delay (1 tick) inbetween each note. You could decide to ignore this delay or correct (recommanded) for it.\n",
    "\n",
    "A MIDI files is composed of timed events of many kinds. For this project, we focus on the events related to notes being played or stopped. Those are MIDI events \"Note Off\" and \"Note On\". Each event is timed with respect with the previous one (dt). The duration of a note is therefore given by the cummulated dt inbetween the event associated to a note's onset and the event associated with the same note's offset.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal code to read MIDI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display(midi_file=\"chorpus/sessiontune0.mid\"):\n",
    "    #index 0 for first track only\n",
    "    events = midi.read_midifile(midi_file)[0] \n",
    "    tick = 0\n",
    "    print(midi_file)\n",
    "    print(\"#\"*40)\n",
    "    print(\"tick \\t pitch \\t message \\t velocity\")\n",
    "    print(\"#\"*40+'\\n')\n",
    "    for n, event in enumerate(events):\n",
    "        tick += event.tick\n",
    "        if event.name in ['Note On', 'Note Off']:\n",
    "            if n<13 or n>len(events)-10:\n",
    "                print(\"%i \\t %i \\t %s \\t %i\"%(tick, event.data[0], event.name, event.data[1]))\n",
    "            if n in range(20,23):\n",
    "                print(\". \\t . \\t . \\t \\t .\")\n",
    "    print(\"#\"*40+'\\n')\n",
    "\n",
    "display()\n",
    "\n",
    "datapath = \"chorpus/\"\n",
    "dataset = {}\n",
    "np.random.seed(seed)\n",
    "\n",
    "for filename in os.listdir(datapath):\n",
    "    if filename.endswith(\"mid\"):\n",
    "        label = filename[:-4]\n",
    "        #Down sample dataset\n",
    "        if np.random.rand() < 0.1:\n",
    "            dataset[label] = {\"T\":[], \"P\": []}\n",
    "print(\"%i/%i melodies retained\"%(len(list(dataset.keys())), len(os.listdir(datapath))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMIDI(midi_file):\n",
    "    events = midi.read_midifile(midi_file)[0] #index 0 for first track only  \n",
    "    T = []\n",
    "    P = []\n",
    "    for n, event in enumerate(events):\n",
    "        #insert your code here...\n",
    "        break\n",
    "    return P, T\n",
    "\n",
    "for label in list(dataset.keys()):\n",
    "    Pseq, Tseq = parseMIDI(datapath+label+\".mid\")\n",
    "    dataset[label]['T']= Tseq\n",
    "    dataset[label]['P']= Pseq\n",
    "\n",
    "for label in np.random.choice(list(dataset.keys()), 2):\n",
    "    print(label)\n",
    "    print(\"MIDI events\")\n",
    "    display(datapath+label+\".mid\")\n",
    "    print(\"Note representation\")\n",
    "    print(dataset[label])\n",
    "    print('/n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Tokenzation and translation to integer\n",
    "### Description\n",
    "\n",
    "Now you should have a \"dataset\" dictionary where each entry corresponds to one melody example from a subset of the full dataset. In turn, every melody example is represented by a dictionary with two sequences: the \"P\" entry being the sequence of pitches and the \"T\" entry, the sequence of durations. Here, you will implement the first steps to translate this dataset in a language that can be read by your ANN models. To achieve that, we will later use the [one-hot encoding scheme](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), which will associate each input (and output) unit of your network to a unique pitch or duration. In your network, these units will then be sequentially activated in the same order as their corresponding pitch/duration in your dataset. And at each note, the model will be trained to approximate the probability distributions of the upcoming pitch and duration.\n",
    "\n",
    "1. Gather in a list of all possible pitches and durations in your dataset. These two lists will be your \"dictionaries\" translating a pitch/duration to a unique integer between 0 and the number of entries (-1) in the pitch/duration dictionary.\n",
    "2. Translate the pitch/duration sequence of each melody to the integer representation.\n",
    "3. Select randomly two melodies and display them in the note representation and translated to integer. Check that the translation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Data reprocessing\n",
    "### Description\n",
    "\n",
    "Before moving to the implementation of your RNN generative model of note transition, you will perform data exploration and modification. This step will allow your model to get the most out of your original data as well as ensuring some features such as transposition invariance. \n",
    "\n",
    "To further improve your model, you might want to reconsider your original data. Many different approaches can be undertaken. Here you are asked to implement the a rare event suppression method and transposition invariance.\n",
    "\n",
    "**Rare event suppression** If a unit in your network is associated to a duration that happens very rarely in your data, a possible simplification consists in removing the melodies conaining these rare events from your dataset. \n",
    "1. Plot and comment the histogram of durations in your entire dataset. Use the labels from the original duration values (rather than the integer representation). \n",
    "2. Based on your observations, remove from your dataset melodies containing very rare durations. Adjust the rejection threshold such that at most 5% of the melodies are dicarded with this step. \n",
    "3. Recompute the duration dictionary based on your filtered dataset and plot the new histogram of durations. \n",
    "\n",
    "**Transposition invariance** A melody is perceived similarly by the human ear if it's shifted up or down by a constant offset (see [transposition](https://en.wikipedia.org/wiki/Transposition_(music))). Here, you will force your model to be transposition invariant. For this you can choose to use one of two orthogonal methods. The first consists in transposing every melody into a common tonality (C Major/A minor). On the other hand, the second consists in transposing each melody in all possible keys. The second method will increase your dataset size by a factor bigger than 11! As the training time is directly related with the dataset size, to save computing time, we recomand that you implement the first option.\n",
    "\n",
    "***Pitch normalization*** \n",
    "1. Use the provided function to create a new dataset where all melodies are transposed to C Major/A minor. \n",
    "2. Plot the histograms of the pitches before and after transposition.\n",
    "\n",
    "Finally, you shoud recompute the tokenization step for your brand new filtered and normalized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transposeDataset(dataset):\n",
    "    transposed_dataset = {}\n",
    "\n",
    "    for label in list(dataset.keys()):\n",
    "        transposed_dataset[label] = {}\n",
    "        score = music21.converter.parse(datapath+label+\".mid\")\n",
    "        key = score.analyze('key')\n",
    "        if key.mode == \"major\":\n",
    "            i = music21.interval.Interval(key.tonic, music21.pitch.Pitch('C'))\n",
    "        elif key.mode == \"minor\":\n",
    "            i = music21.interval.Interval(key.tonic, music21.pitch.Pitch('A'))\n",
    "        i = i.semitones\n",
    "        transposed_dataset[label]['P'] = [p+i for p in dataset[label]['P']]\n",
    "        transposed_dataset[label]['T'] = dataset[label]['T']\n",
    "        \n",
    "    return transposed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How much did you reduce the space of possible durations with your rare event suppression procedure? What is the impact in term of training time?\n",
    "    \n",
    "**Answer**: \n",
    "\n",
    "**Question**: Explain why and how both transposition to a single tonality and to all possible tonalities can yield transposition invariance in your model. \n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Question**: In terms of number of dimensions, what would have been the impact of keeping the joint represenation of a note duration and pitch (vs separating the two features)? \n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question**: How would the distribution of notes in the joint representation differ from the distributions in the pitch and duration only representations (as shown by your histograms above)?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: One-hot and zero-padding\n",
    "### Description\n",
    "\n",
    "The last few steps to be able to feed (note by note) your music scores to a RNN model are\n",
    "1. Transform the sequence of integers into a sequence of vectors in the one-hot encoding scheme.\n",
    "2. Homegenize the length of each sequence\n",
    "\n",
    "**One-hot encode** You can implement your own integer to one-hot code. For this use the numpy library and the suitable datatype. Alternatively, Keras provides a [built in one-hot encoding scheme](https://keras.io/utils/#to_categorical). After this step, for each melody, you should have two similar length matrices with one representing the sequence of note pitches and the other the sequence of note durations.\n",
    "1. Code the integer representation into matrices of one-hot vectors\n",
    "\n",
    "**Zero-padding** Most deep learning libraries, including Keras, require that you specify the shape of the input data. As such, you cannot have variable length inputs per-se. The trick to handle them is [zero-padding](https://keras.io/preprocessing/sequence/#pad_sequences). Provided with the [Masking Layer](https://keras.io/layers/core/#masking) Keras will then ignore time steps where all inputs are 0.\n",
    "1. Extend the end of each melody matrices with vectors containing only zeros until all melodies have the same size. For simplicity, use the Keras pad_sequences function. There might be some rare very long melodies in your dataset, a possible choice is to set the 'maxlen' argument of pad_sequences to the mean+2std of the melody lengths. This ensure that 95% of your melodies would end before being truncated.\n",
    "2. Check and display the dimension of the resulting tensors. Typically, the tensor representing the sequence of pitches should be of shape [number of melodies x number of notes x number of entries in the pitch dictionary] and the tensor representing the sequence of durations [number of melodies x number of notes x number of entries in the duration dictionary]\n",
    "\n",
    "To check if this step was done correctly use the provided plotMelody function. For two randomly selected melodies, call the plotMelody function before and after one-hot/zero-pad. You should obtain the same plots (or slightly truncated if you randomly selected one belonging to 5% longest).\n",
    "\n",
    "Finally, we strongly suggest that you save your preprocessed data now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Building and training a generative model of  note transition with the Keras functional API\n",
    "### Description\n",
    "\n",
    "The Keras Sequential model is very efficient for fast prototyping, however suffers from a lack of design liberty. The Keras Functional API is only slightly more complex but allows for more control on the different parts of your model. You will be using the Functional API for the rest of this project. Please read carefully the [documentation](https://keras.io/getting-started/sequential-model-guide/). \n",
    "\n",
    "Here, you will be constructing your RNN model of note transition. To achieve that, your model should be trained to approximate the probability distribution of the upcoming note (observed as two output softmax layers - one for pitch and one for duration) given the current note (given as input) and the model internal representation of the history of notes (its hidden state). To achieve that, set the Keras RNN layer argument \"return_sequences\" to True when building your model. Doing this will give you an output for each note, rather than a single output at the end of the sequence. During training, the target at each time step is the next note pitch and duration.\n",
    "\n",
    "You are free to use your own architecture but make sure to use at least 128 hidden units, Masking, and dropout to prevent overfitting. Your model should have two output (softmax) layers, one related to the pitch predictions that you should name `YP`, and the other related to the duration predictions `YT`. You should monitor both the loss (categorical_crossentropy) and the accuracy. For your best model, you should reach 40%/75% accuracy on the pitch/duration predictions.\n",
    "\n",
    "1. Train your first model composed of SimpleRNN cells for min 250 epochs and save the model/parameters that lead to the higher accuracy on the validating set. Use a validation split of 0.2. Try  If you correctly named the output layers (`YP` and `YT`) use the provided `plotLearningCurves(History)` function to plot the learning curves by giving the History object returned by the Keras fit function as argument.  \n",
    "2. Train a second model where you replaced the SimpleRNN units with GRUs (LSTM unit equivalent that requires less parameters). Make sure you use the same hyperparameters as before. Plot the learning curves.\n",
    "3. On a single plot, adapt the `plotLearningCurves(History)` function to compare the validation learning curves of the simpleRNN and GRU models. In other words, plot the validation loss vs epoch for pitch/duration for the simpleRNN and GRU models. Do the same for accuracies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(dictionaries, batch_length, dropout=0.2, activation='GRU', Hsize=128):\n",
    "    X = dict()\n",
    "    H = dict()\n",
    "    M = dict()\n",
    "    Y = dict()\n",
    "    \n",
    "    X['T'] = Input(shape=(batch_length, len(dictionaries['T'])), name=\"XT\")\n",
    "    X['P'] = Input(shape=(batch_length, len(dictionaries['P'])), name=\"XP\")\n",
    "    \n",
    "    M['T'] = Masking(mask_value=0., name=\"MT\")(X['T'])\n",
    "    M['P'] = Masking(mask_value=0., name=\"MP\")(X['P'])\n",
    "    \n",
    "    H['1'] = Concatenate(name=\"MergeX\")([M['T'], M['P']])\n",
    "    if activation == 'GRU':\n",
    "        #Your hidden layer(s) architecture with GRU\n",
    "    elif activation == 'LSTM':\n",
    "        #Your hidden layer(s) architecture with LSTM (For your own curiosity, not required for the project)\n",
    "    elif activation == 'RNN':\n",
    "        #Your hidden layer(s) architecture with SimpleRNN\n",
    "\n",
    "    Y['T'] = TimeDistributed(Dense(len(dictionaries['T']), activation='softmax'), name='YT')(#Input(s) to duration output layer)\n",
    "    Y['P'] = TimeDistributed(Dense(len(dictionaries['P']), activation='softmax'), name='YP')(#Input(s) to pitch output layer)\n",
    "    \n",
    "    model = Model(inputs = [X['T'], X['P']], outputs = [Y['T'], Y['P']])\n",
    "    opt = Adam() \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=opt,\n",
    "        metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "RNNmodel = buildModel(dictionaries, \n",
    "                      batch_length=,#Put here the number of notes (timesteps) you have in your Zero-padded matrices\n",
    "                      activation='RNN')\n",
    "RNNmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What are the best prediction accuracies you obtained? With which model?\n",
    "    \n",
    "**Answer**: \n",
    "\n",
    "**Question**: In a few sentences, comment on the plot comparing learning curves for the two activation function.\n",
    "    \n",
    "**Answer**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 6: Visualizing the model\n",
    "### Description\n",
    "\n",
    "1. Using the model.predict and plotMelody functions, show the outputs of your best model when fed with 4 randomly selected melodies. Report the prediction accuracies as well.\n",
    "2. For the same melodies, plot the hidden state evolution accross time steps. Construct a 2 dimensional matrix with the first dimension being time and the second the concatenation of all hidden unit activations and plot it using the provided `plotMelody` function. To get the hidden states of your network, you can modify the `buildModel` function so that you have another computation graph (another model with the same layers, but the output layer) returning you the hidden states. However, other approaches are equally valid.  \n",
    "3. For each hidden unit $i$ and accross the entire dataset, compute the average absolute change in activation $\\Delta A_i$. Be careful not to take masked timesteps into account. Plot the histogram (50 bins) of this average for the RNN and GRU models on the same figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How does the hidden state dynamics explain the performance of the RNN vs GRU models? Explain in term of temporal dependencies and base your explanation on figures obtained in 6.2 and 6.3.\n",
    "    \n",
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 7: Generation of new melodies\n",
    "### Description\n",
    "\n",
    "1. Use your trained best model to generate new melodies. You can do that by starting with a random note, feeding it to your network and sample the output predictions in order to select the next note. Then add this note to your iteratively growing melodies. For your convenience, we provided the `sample` and `writeMIDI` functions. You can use the first in order to perform temperature sampling. The `writeMIDI` function allows you to write a MIDI file from the integer representation.\n",
    "2. Try different sampling temperatures (e.g. 0.5, 1., 1.5) and observe the effect on your generated melodies.\n",
    "3. Generate 32 melodies with both the RNN and GRU models\n",
    "4. Listen to 4 randomly chosen melodies (2 from the GRU model and 2 from the RNN model). To play in a notebook, use a similar approach as the one shown below assuming you have the files `GRU_1.mid` and `RNN_1.mid` at the same level as this notebook. For your own curiosity, you can also directly listen to the MIDI files with your favorite MIDI player but make sure that we can listen to the 4 extracts directly from this notebook (without rerunning it). You might have to use the \"Trust notebook\" button (in the top right corner) to see the players. This currently only works in jupyter notebook (not jupyter lab). If you have trouble achieving that, upload the MIDI files with your submission.\n",
    "5. (Optional) If you have a big enough computing power, try adding multiple and bigger layers. You could also try to improve your model with different add-ons, e.g. make your model learn when a melody ends, extend it to other datasets, ...\n",
    "6. (Optional) If you are happy with your results submit a 1 hour long generated MIDI file to the [AI-generated music challenge](https://www.crowdai.org/challenges/ai-generated-music-challenge). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv3914'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv3914');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('GRU')\n",
    "music21.midi.translate.midiFilePathToStream('GRU_1.mid').show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv7631'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv7631');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('RNN')\n",
    "music21.midi.translate.midiFilePathToStream('RNN_1.mid').show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the effect of the sampling temperature on the generated melodies?\n",
    "    \n",
    "**Answer**: \n",
    "\n",
    "**Question**: Are the generated melodies from the RNN model different from the one generated by the GRU model? If yes, what are the main differences?\n",
    "\n",
    "**Answer**: \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
